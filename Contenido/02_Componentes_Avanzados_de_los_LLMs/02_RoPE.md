# RoPE: codificaci칩n posicional rotatoria para transformers

## Resumen

**游 Contexto r치pido: En esta secci칩n vas a ver los t칠rminos Rotary Position Embedding y Rotary Position Encoding. Queremos que sepas que ambos se refieren a la misma t칠cnica: RoPE. No te preocupes por la diferencia en los nombres, es solo una cuesti칩n de estilo.** En la mayor칤a de los art칤culos y recursos t칠cnicos se usa ***Rotary Position Embedding***, pero el concepto es el mismo: una forma de incorporar la posici칩n en los tokens a trav칠s de rotaciones en el espacio vectorial.

El **Rotatory Position Encoding (RoPE)** es una t칠cnica esencial para lograr que los modelos Transformers, como GPT-2, reconozcan la posici칩n espec칤fica de cada token dentro de una secuencia. Implementar RoPE mejora notablemente la precisi칩n del modelo reduciendo la carga computacional necesaria durante su entrenamiento.

## 쯇or qu칠 los transformers necesitan RoPE?

Los Transformers no tienen una forma natural de interpretar la ubicaci칩n exacta de los tokens en una secuencia. Esto implica que para modelos sin encoding posicional podr칤a resultar id칠ntica la relaci칩n entre tokens cercanos y aquellos muy separados, llevando a una mayor demanda de c칩mputo para el entrenamiento. RoPE soluciona esto almacenando informaci칩n sobre la posici칩n mediante el 치ngulo del embedding.

## 쮺칩mo funciona RoPE en la codificaci칩n de posici칩n?

En RoPE, cada palabra o token posee un embedding que puede visualizarse en un espacio vectorial bidimensional. Aqu칤 es donde RoPE explota la idea del 치ngulo:

* Cada embedding tiene un 치ngulo asociado respecto al eje x.
* Este 치ngulo (theta) codifica la posici칩n espec칤fica del token en la secuencia.
* A medida que avanzamos en la secuencia se rota el embedding seg칰n el 치ngulo theta, permitiendo al modelo retener con precisi칩n la ubicaci칩n del token.

Adem치s, RoPE asegura consistencia manteniendo invariantes las distancias relativas entre embeddings, siempre que la cantidad de tokens entre elementos objetivo permanezca constante.

## 쮺u치l es la matem치tica detr치s de RoPE?

RoPE utiliza matrices de rotaci칩n del 치lgebra lineal, en particular empleando funciones trigonom칠tricas:

* Coseno y menos seno en la primera fila.
* Seno y coseno en la segunda fila.

Esta matriz rota los vectores embeddings para codificar su posici칩n relativa dentro de secuencias m치s grandes. Dado que los embeddings t칤picos tienen m칰ltiples dimensiones, se dividen en pares dimensionales para aplicar matrices de rotaci칩n eficientemente.

Para reducir la carga computacional, RoPE no multiplica directamente grandes matrices, sino que ejecuta operaciones vectoriales:

* Divisi칩n del embedding en grupos de dos dimensiones.
* Aplicaci칩n individual de la matriz de rotaci칩n por pares.
* Operaciones vectoriales m치s simples (coseno, seno, suma y producto punto).

## 쮺칩mo mejora RoPE el desempe침o del modelo?

Integrar RoPE facilita que el modelo entienda con exactitud la distancia entre tokens, aumentando la calidad y velocidad del entrenamiento. La reducci칩n r치pida de la p칠rdida durante el entrenamiento con RoPE es un claro indicador de su eficiencia y de c칩mo favorece el aprendizaje m치s 치gil del modelo.

Adem치s, ayuda al modelo a discernir qu칠 tan pr칩ximos est치n los tokens en la secuencia a trav칠s del producto punto entre vectores, facilitando interpretaciones m치s exactas durante el entrenamiento y la inferencia.