{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b839b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: xformers in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (0.0.32.post2)\n",
      "Requirement already satisfied: np in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from xformers) (2.3.2)\n",
      "Requirement already satisfied: torch==2.8.0 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from xformers) (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from sympy>=1.13.3->torch==2.8.0->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from jinja2->torch==2.8.0->xformers) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install einops xformers np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f085e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import ModuleList\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch import nn, einsum, broadcast_tensors\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f88d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d3ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, n):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbe673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nx, nf):\n",
    "        '''\n",
    "        nx: Numero de datos de entrada.\n",
    "        nf: Numero de filtros. (Canales de salida).\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        #Inicializando una matriz vacia de pesos del tamaño (nx)X(nf)\n",
    "        w = torch.empty(nx, nf)\n",
    "        #Inicializando los pesos con una distribución normal.\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        #Calculando los pesos y sesgos encodeandos usando nn.Parameter\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''x:Tensor de entrada.'''\n",
    "        #El tamaño de la salida es la suna de la segunda dimensión de X y el número de filtros nf.\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        # Producot punto Q,K(Transpuesta) y V\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)# x.view ayuda a calcular la transpuesta.\n",
    "        x = x.view(*size_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7fd405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
    "        super().__init__()\n",
    "        self.c_fc    = Conv1D(d_model, nx)\n",
    "        self.c_proj  = Conv1D(nx, d_model)\n",
    "        self.act     = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53f07352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def broadcat(tensors, dim = -1):\n",
    "    broadcasted_tensors = broadcast_tensors(*tensors)\n",
    "    return torch.cat(broadcasted_tensors, dim = dim)\n",
    "\n",
    "def rotate_half(x):\n",
    "    '''\n",
    "    The initial step of our roformer includes use of In order to generalize our results in 2D to any xi ∈ Rd\n",
    "    where d is even, we divide the d-dimension space into d/2\n",
    "    sub-spaces and combine them in the merit of the linearity of the inner product, turning f{q,k} into\n",
    "    \n",
    "    Lo anterior fue un extracto del artículo que implica dividir en d/2\n",
    "    '''\n",
    "    x = rearrange(x, '... (d r) -> ... d r', r = 2)\n",
    "    x1, x2 = x.unbind(dim = -1)\n",
    "    x = torch.stack((-x2, x1), dim = -1)\n",
    "    return rearrange(x, '... d r -> ... (d r)')\n",
    "\n",
    "def apply_rotary_emb(freqs, t, start_index = 0, scale = 1., seq_dim = -2):\n",
    "    '''\n",
    "    Una función para aplicar las rotaciones del embedding, obteniendo primero la dimensión de rotación y la longitud de la secuencia\n",
    "    obteniendo el índice final sumando el índice inicial y la dimensión de rotación como se mencionó anteriormente,\n",
    "    la t izquierda, t y t derecha con el segmento de token anterior, durante el segmento de token y después del segmento de token\n",
    "    aplica la rotación del embedding a la porción central de t.\n",
    "\n",
    "    La rotación implica una combinación de operaciones de coseno y seno utilizando las frecuencias y el factor de escala especificados.\n",
    "    '''\n",
    "    rot_dim, seq_len = freqs.shape[-1], t.shape[seq_dim]\n",
    "    freqs = freqs[-seq_len:].to(t)\n",
    "    end_index = start_index + rot_dim\n",
    "    t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n",
    "    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n",
    "    return torch.cat((t_left, t, t_right), dim = -1)\n",
    "\n",
    "def apply_learned_rotations(rotations, t, start_index = 0, freq_ranges = None):\n",
    "    '''\n",
    "    Aprendizaje de rotaciones mediante el manejo de frecuencias mediante la ampliación de las rotaciones,\n",
    "    esta reorganización ayuda a combinar las rotaciones en una sola, ahora se repiten las rotaciones replicando\n",
    "    las rotaciones y luego se aplican las rotaciones de embeddings.'''\n",
    "    if exists(freq_ranges):\n",
    "        rotations = einsum('..., f -> ... f', rotations, freq_ranges)\n",
    "        rotations = rearrange(rotations, '... r f -> ... (r f)')\n",
    "\n",
    "    rotations = repeat(rotations, '... n -> ... (n r)', r = 2)\n",
    "    return apply_rotary_emb(rotations, t, start_index = start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beae4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        theta = 10000,\n",
    "        max_freq = 10,\n",
    "        num_freqs = 1,\n",
    "        interpolate_factor = 1.,\n",
    "        theta_rescale_factor = 1.,\n",
    "    ):\n",
    "        '''Esta es un constructor del RoPE\n",
    "        theta: El angulo de rotación\n",
    "        max_freq: La frecuencia maxima de rotación\n",
    "        num_freq: El numero de veces la frecuencia necesaria para ser iterado.\n",
    "        interpolate factor: Un factor usado para controlar el valor del Positional Embedding si es mayor o menor.\n",
    "        theta_rescale_factor: Como el valor theta decae a medida que aprende necesitamos reescalarlo en ese proceso.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        theta *= theta_rescale_factor ** (dim / (dim - 2))\n",
    "\n",
    "\n",
    "        freqs = 1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n",
    "\n",
    "        self.cache = dict()\n",
    "        self.cache_scale = dict()\n",
    "        self.freqs = nn.Parameter(freqs)\n",
    "\n",
    "\n",
    "        # Dimesión base para la sequencia.\n",
    "        self.default_seq_dim = -2\n",
    "\n",
    "        # Factores de interpolación.\n",
    "        assert interpolate_factor >= 1.\n",
    "        self.interpolate_factor = interpolate_factor\n",
    "\n",
    "        # xpos\n",
    "        self.register_buffer('scale', None)\n",
    "\n",
    "\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.register_buffer('scale', scale)\n",
    "\n",
    "    def get_seq_pos(self, seq_len, device, dtype, offset = 0):\n",
    "        '''\n",
    "        La función para obtener la sequencial posicional del embeding usando torch.arange\n",
    "        que usa [end-start]/start dividido por el factor de interpolación. para controlar\n",
    "        su valor.\n",
    "        '''\n",
    "        return (torch.arange(seq_len, device = device, dtype = dtype) + offset) / self.interpolate_factor\n",
    "\n",
    "    def rotate_queries_or_keys(self, t, seq_dim = None, offset = 0, freq_seq_len = None):\n",
    "        '''Función para operar la rotación sobre las queries y keys.'''\n",
    "        seq_dim = default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "\n",
    "        device, dtype, seq_len = t.device, t.dtype, t.shape[seq_dim]\n",
    "\n",
    "        if exists(freq_seq_len):\n",
    "            assert freq_seq_len >= seq_len\n",
    "            seq_len = freq_seq_len\n",
    "\n",
    "        freqs = self.forward(lambda: self.get_seq_pos(seq_len, device = device, dtype = dtype, offset = offset), cache_key = f'freqs:{seq_len}|offset:{offset}')\n",
    "\n",
    "        if seq_dim == -3:\n",
    "            freqs = rearrange(freqs, 'n d -> n 1 d')\n",
    "\n",
    "        return apply_rotary_emb(freqs, t, seq_dim = seq_dim)\n",
    "\n",
    "    def forward(self, t, cache_key = None):\n",
    "        '''Función para propagar el valor T.'''\n",
    "        should_cache = exists(cache_key)\n",
    "\n",
    "        if should_cache and cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        if callable(t):\n",
    "            t = t()\n",
    "\n",
    "        freqs = self.freqs\n",
    "\n",
    "        freqs = einsum('..., f -> ... f', t.type(freqs.dtype), freqs) #Convirtiendo las frequencias en la transpuesta.\n",
    "        freqs = repeat(freqs, '... n -> ... (n r)', r = 2)\n",
    "\n",
    "        if should_cache:\n",
    "            self.cache[cache_key] = freqs\n",
    "\n",
    "        return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3164b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n",
    "        '''Función de construcción\n",
    "        Params:\n",
    "        d_model:Dimensión que necesita ser ingresada en el modelo.\n",
    "        n_head:La cantidad de heads de atención.\n",
    "        n_ctx:Buffer para guardar los registros del sesgo.\n",
    "        d_head:Dimesión de salida para el head.\n",
    "        bias:Un booleano para saber si incluir el sesgo.\n",
    "        scale: Escalar y estabilidad númerica (sqrt(dk))\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.n_head  = n_head\n",
    "        self.d_model = d_model\n",
    "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
    "        self.scale   = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.c_proj  = Conv1D(d_model, d_model)\n",
    "        self.rotate = RotaryEmbedding(dim=32)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Diviendo en la cantidad de heads y retornando.\n",
    "        return shape [`batch`, `head`, `sequence`, `features`]\n",
    "        \"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head)\n",
    "        x = x.view(*new_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def _attn(self, q, k, v, attn_mask=None):\n",
    "        \"\"\"Función de antención principal.\n",
    "        Que calcula usando la formula de producto punto de atención.\"\"\"\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))# producto punto de Q*K(t)\n",
    "        if self.scale: scores = scores/math.sqrt(v.size(-1))# escalandola por sqrt(dk)\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        if attn_mask is not None: scores = scores + attn_mask# agregando los valores con la mascara de atención.\n",
    "        scores  = self.softmax(scores)# añadiendo los valores de softmax\n",
    "        scores  = self.dropout(scores) # función de dropout 0.1\n",
    "        outputs = torch.matmul(scores, v) # Multiplicación final del puntaje por V.\n",
    "        return outputs\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        # Combinando todas las heads en una sola.\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Función de para calcular atención, separar las heads y combinarlas de nuevo.'''\n",
    "        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n",
    "        q, k, v  = x.split(self.d_model, dim=2)\n",
    "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        q = self.rotate.rotate_queries_or_keys(q)\n",
    "        k = self.rotate.rotate_queries_or_keys(k)\n",
    "        out      = self._attn(q, k, v)\n",
    "        out      = self.merge_heads(out)\n",
    "        out      = self.c_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f8ad02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn        = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
    "        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n",
    "        self.ln_1        = LayerNorm(d_model)\n",
    "        self.ln_2        = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.feedforward(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab9d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n",
    "        '''\n",
    "        nlayer: La cantidad de veces que queremos multiplicar el Transformer.\n",
    "        n_ctx: El contexto, la cantidad total de tokens que puede ver en el pasado de las palabras.\n",
    "        d_model:Dimesionos del modelo.\n",
    "        vcb_sz:El tamaño del vocabulario usado en el entrenamiento.\n",
    "        '''\n",
    "        super(GPT2, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        block        = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n",
    "        self.h       = _get_clones(block, 12)\n",
    "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
    "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop    = nn.Dropout(0.1)\n",
    "        self.ln_f    = LayerNorm(d_model)\n",
    "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        '''Inicialización de los pesos.'''\n",
    "        self.out.weight = self.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        '''Inicialización con la media y S.D.'''\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                '''Data Bias zero'''\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, src, labels=None, pos_ids=None):\n",
    "        '''\n",
    "        Añadir el embedding posicional, dropping y añadiendo los inputs\n",
    "        usados por la función de perdida y finalmente añadiendo la salida y la\n",
    "        perdida.\n",
    "        '''\n",
    "        if pos_ids is None:\n",
    "            pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
    "        pos_ids = pos_ids.to(src.device)  # Asegurarse que los pos_ids están en el mismo device.\n",
    "        inp = self.drop((self.wte(src) + self.wpe(pos_ids)))\n",
    "        for i in range(self.nlayers): inp = self.h[i](inp)\n",
    "        inp     = self.ln_f(inp)\n",
    "        logits  = self.out(inp)\n",
    "        outputs = (logits,) + (inp,)\n",
    "\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "            return loss.mean()\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda8a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import time\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "238cf17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84fe2f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descarga completada: gpt2-pytorch_model_rope.bin\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://huggingface.co/Zuckerbird/RoPE-gpt2/resolve/main/pytorch_model.bin\"\n",
    "output = \"gpt2-pytorch_model_rope.bin\"\n",
    "urllib.request.urlretrieve(url, output)\n",
    "print(\"Descarga completada:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c91fa83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model.state_dict()\n",
    "state_dict = torch.load(\"./gpt2-pytorch_model_rope.bin\")\n",
    "\n",
    "old_keys = []\n",
    "new_keys = []\n",
    "for key in state_dict.keys():\n",
    "    if \"mlp\" in key: #El diccionario de estado para el MLP feedforward debe ser cambiado por mlp\n",
    "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
    "        new_keys.append(new_key)\n",
    "        old_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "698180c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for old_key, new_key in zip(old_keys, new_keys):\n",
    "    state_dict[new_key]=state_dict.pop(old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15930872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "        (rotate): RotaryEmbedding()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dec4ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "705270f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño total de GPT2 sin alteraciones es: 497760000 bytes o 474.70 MB\n"
     ]
    }
   ],
   "source": [
    "size_bytes = total_params * 4\n",
    "size_mb = size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"El tamaño total de GPT2 sin alteraciones es: {size_bytes} bytes o {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2665cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "context = torch.tensor([tokenizer.encode(\"The planet earth is a beautiful\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "629cdf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(context, ntok=550):\n",
    "    start_time = time.time()\n",
    "    for _ in range(ntok):\n",
    "        out = model(context)\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = -np.inf\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    return context, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84301e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, inference_time = generate(context, ntok=40)\n",
    "decoded_output = tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c3512de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 3.5004 seconds\n",
      "Generated Output: The planet earth is a beautiful plant thankfully�auder Question 24 architect interestsitic dart Tarleysefully Terminatorrieving ✓ 1925ussenquart globallyvir agonginsulture Bahrain exqu idi Nig Tactical corpse those fear seekers FulUsAccept hysterical forgiveness Casual distilled\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"Generated Output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f385bf",
   "metadata": {},
   "source": [
    "[REGRESAR](../03_Integracion_de_Rope_en_GPT-2.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
