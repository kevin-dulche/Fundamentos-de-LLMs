{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ad4e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: xformers in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (0.0.32.post2)\n",
      "Requirement already satisfied: np in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from xformers) (2.3.2)\n",
      "Requirement already satisfied: torch==2.8.0 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from xformers) (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from torch==2.8.0->xformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from sympy>=1.13.3->torch==2.8.0->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kevin\\desktop\\fundamentos-de-llms\\venv\\lib\\site-packages (from jinja2->torch==2.8.0->xformers) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install einops xformers np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8167d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import ModuleList\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch import nn\n",
    "\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf59ef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589d7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, n):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9668081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nx, nf):\n",
    "        '''\n",
    "        nx: Numero de datos de entrada\n",
    "        nf: Numero de filtros. (Canales de salida).\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        #Inicializando matriz vacia de pesos del tamaño (nx)X(nf)\n",
    "        w = torch.empty(nx,nf)\n",
    "        #Calculando los pesos con una distribución normal.\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        #Calculando los pesos y sesgos encodeandos usando nn.Parameter.\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''x:Tensor de entrada.'''\n",
    "        #El tamaño de la salida es la suma de la segunda dimensionm de X y el numero de filtros nf.\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        #Producto punto Q,K(Transpuesta) y V\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight) # x.view ayuda a calcular la transpuesta.\n",
    "        x = x.view(*size_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229c7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
    "        super().__init__()\n",
    "        self.c_fc     = Conv1D(d_model, nx)\n",
    "        self.c_proj   = Conv1D(nx, d_model)\n",
    "        self.act      = F.gelu\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9f5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n",
    "        '''Función de construcción\n",
    "        Params:\n",
    "        d_model: Dimensión que necesita ser ingresada en el modelo.\n",
    "        n_head: La cantidad de heads de atención\n",
    "        n_ctx: Buffer para guardar los registros del sesgo.\n",
    "        scale: Escalar y estabilidad númerica (sqrt(dk))\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.c_attn = Conv1D(d_model, d_model*3)\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.c_proj = Conv1D(d_model, d_model)\n",
    "\n",
    "    def split_heads(self,x):\n",
    "        \"\"\"\n",
    "        Dividiend en la cantidad de heads y retornando.\n",
    "        return shape ['Barch', 'head', 'sequence', 'features']\n",
    "        \"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head)\n",
    "        x = x.view(*new_shape)\n",
    "        return x.permute(0,2,1,3)\n",
    "\n",
    "    def _attn(self, q, k, v, attn_mask=None):\n",
    "        \"\"\"Función de atención principal.\n",
    "        Que calcula usando la formula de producto de punto de atención.\"\"\"\n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) #producto punto de Q*K(t)\n",
    "        if self.scale: scores = scores/math.sqrt(v.size(-1)) #escalandolo por sqrt(dk)\n",
    "        nd, ns = scores.size(-2), scores.size(-1)\n",
    "        if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores = self.softmax(scores)\n",
    "        scores = self.dropout(scores)\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "\n",
    "    def merge_heads(self,x):\n",
    "        x = x.permute(0,2,1,3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Función de para calcular atención, separar los heads y combinarlos de nuevos.'''\n",
    "        x = self.c_attn(x)\n",
    "        query, key, value = x.split(self.d_model, dim=2)\n",
    "        query, key, value = self.split_heads(query), self.split_heads(key), self.split_heads(value)\n",
    "        out = self._attn(query, key, value)\n",
    "        out = self.merge_heads(out)\n",
    "        out = self.c_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739001ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
    "        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.feedforward(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vob_size=50257): # GPT-3 usa nlayers=96, n_ctx=2048, d_model=12288\n",
    "        '''nlayer: La cantidad de veces que queremos multiplicar el transformer.\n",
    "        n_ctx: El contexto, la cantidad total de tokens que puede ver en el pasado de las palabras.\n",
    "        d_model: Dimesiones del modelo\n",
    "        vob_size: Tamaño del vocabulario usado en el entrenamiento.'''\n",
    "        super(GPT2, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        block = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n",
    "        self.h = _get_clones(block, 12)\n",
    "        self.wte = nn.Embedding(vob_size, d_model)\n",
    "        self.wpe = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.ln_f = LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vob_size, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        '''Incilizacion de los pesos'''\n",
    "        self.out.weight = self.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        '''Iniciliazación con la medida y S.D.'''\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                '''Data Bias Zero'''\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, src, labels=None, pos_ids=None):\n",
    "        '''Añadir el embedding posicional, dropping y añadiendo los inputs usados por la función\n",
    "        de perdida y finalmente añadiendo la salida y la peridida.'''\n",
    "\n",
    "        if pos_ids is None:\n",
    "            pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
    "\n",
    "        pos_ids = pos_ids.to(src.device)\n",
    "\n",
    "        inp = self.drop((self.wte(src) + self.wpe(pos_ids)))\n",
    "        for i in range(self.nlayers): inp = self.h[i](inp)\n",
    "\n",
    "        inp = self.ln_f(inp)\n",
    "        logits = self.out(inp)\n",
    "\n",
    "        outputs = (logits,) + (inp,)\n",
    "\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "            return loss.mean()\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e9461bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import time\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4730387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "146ad9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descarga completada: gpt2-pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\"\n",
    "output = \"gpt2-pytorch_model.bin\"\n",
    "urllib.request.urlretrieve(url, output)\n",
    "print(\"Descarga completada:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4256d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model.state_dict()\n",
    "state_dict = torch.load(\"./gpt2-pytorch_model.bin\", weights_only=False)\n",
    "\n",
    "old_keys = []\n",
    "new_keys = []\n",
    "\n",
    "for key in state_dict.keys():\n",
    "    if \"mlp\" in key:\n",
    "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
    "        new_keys.append(new_key)\n",
    "        old_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b469b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for old_key, new_key in zip(old_keys, new_keys):\n",
    "    state_dict[new_key] = state_dict.pop(old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78bf14ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f391426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0818996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño total de GPT2 sin alteraciones es: 497759232 bytes o 474.70 MB\n"
     ]
    }
   ],
   "source": [
    "size_bytes = total_params * 4\n",
    "size_mb = size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"El tamaño total de GPT2 sin alteraciones es: {size_bytes} bytes o {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46edca37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6ec3499df949b4a38723018315fca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\Desktop\\Fundamentos-de-LLMs\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kevin\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f24cc5cab14090a7181e58d03fd015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cc89a71038461ebd9d77f92971f689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07753fade88c4879b5350720c76c7237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf262d33cf42447c81b01c3dfa171e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "context = torch.tensor([tokenizer.encode(\"The planet earth is a beautiful\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eb15ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(context, ntok=550):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(ntok):\n",
    "        out = model(context)\n",
    "        logits = out[0][:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = -np.inf\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    return context, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a16f3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, inference_time = generate(context, ntok=40)\n",
    "decoded_output = tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da009ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 3.1711 seconds\n",
      "Generated Output: The planet earth is a beautiful place where people of your choice. Earth in your where they've a a, in you are a in an is Your\n",
      "\n",
      "<|endoftext|>\" a user \"I say .the name\"isplanet a\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"Generated Output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd854dd",
   "metadata": {},
   "source": [
    "[REGRESAR](../01_Construccion_de_GPT-2.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
