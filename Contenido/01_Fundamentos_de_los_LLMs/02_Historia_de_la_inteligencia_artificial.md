# Historia de la inteligencia artificial desde Turing hasta GPT-4

## Resumen

La inteligencia artificial (IA) y específicamente los modelos GPT han revolucionado la forma en que interactuamos con la tecnología. Desde las bases sentadas por Alan Turing en los años cuarenta, hasta la aparición de ChatGPT, es esencial entender cómo ha evolucionado este campo y cómo influye en nuestro día a día.

## ¿Cómo inició la Inteligencia Artificial?

El origen de la inteligencia artificial se remonta a los años cuarenta con la teoría computacional desarrollada por Alan Turing. Este científico propuso la capacidad de que máquinas comprendieran el conocimiento humano mediante comandos binarios, y posteriormente, con su prueba de Turing, estableció las primeras bases para evaluar si una máquina podía emular la conversación humana.

Poco tiempo después, los investigadores McCulloch y Pitts introdujeron la primera neurona artificial, una innovación crucial para el posterior desarrollo del campo.

## ¿Qué fue el AI Winter y cómo afectó al desarrollo tecnológico?

Entre 1970 y principios del año 2000 ocurrió el denominado AI Winter, un periodo caracterizado por significativa reducción de recursos e inversiones en IA, principalmente debido al fracaso de varios proyectos gubernamentales que no cumplían con las expectativas. Durante estos años, agencias destacadas como DARPA vieron necesario cortar significativamente sus financiaciones debido a críticas y escepticismos generalizados.

Sin embargo, pese a estos desafíos, el trabajo continuó, destacándose aportes clave como el desarrollo de las Convolutional Neural Networks (CNN) por parte del joven Yann LeCun en 1989. Este avance marcó un punto importante en la mejora de procesos de reconocimiento de imágenes y datos visuales.

## ¿Cómo han evolucionado los modelos GPT?

NVIDIA, con su lanzamiento del software CUDA en 2007, capacitó a los desarrolladores para realizar cálculos complejos mediante tarjetas gráficas. Esta innovación propició en 2011 el entrenamiento del modelo AlexNet, utilizando hardware que inicialmente estaba destinado al gaming. Avances posteriores liderados por científicos como Ilya Sutskever y Geoffrey Hinton contribuyeron al lanzamiento de transformers y sentaron las bases para la aparición del primer GPT en 2018.

Desde entonces, OpenAI ha protagonizado la evolución acelerada de estos  GPT (Generative Pre-trained Transformer):

* **GPT-1 (2018)**: introdujo los transformers con apenas 117 millones de parámetros.
* **GPT-2 (2019)**: incrementó la complejidad hasta los 1.5 billones de parámetros.
* **GPT-3 (2020)**: dio un gran salto, alcanzando 175 billones de parámetros.
* **ChatGPT y GPT-3.5**: facilitaron a nivel masivo el acceso a estos modelos, mostrando un impresionante desempeño en interacciones conversacionales.
* **GPT-4**: agregó el factor humano en su entrenamiento mediante RLHF (Reinforcement Learning with Human Feedback), llegando a impresionantes 1.7 trillones de parámetros.
* **DeepSeek y LLAMA**: modelos recientes que han optimizado sus capacidades con técnicas avanzadas frente a las limitaciones tecnológicas actuales.

## ¿Cuál es el presente y futuro cercano de la IA?

Hoy en día, el impacto de la inteligencia artificial se evidencia diariamente, modificando la interacción entre humanos y tecnologías digitales. Empresas líderes como Meta y Google están integrando estos avances en diversos productos, indicando la relevancia creciente de los grandes modelos de lenguaje.

La rápida escalabilidad de estos modelos plantea inquietudes en torno al acercamiento hacia la Inteligencia Artificial general (Artificial General Intelligence, AGI), y cómo estos progresos continuarán transformando múltiples aspectos del conocimiento y las interacciones humanas.

[REGRESAR](../01_Fundamentos_de_los_LLMs/Intro.md)