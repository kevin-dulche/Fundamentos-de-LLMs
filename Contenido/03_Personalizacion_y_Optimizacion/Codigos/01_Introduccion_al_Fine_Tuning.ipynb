{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxJYzBJncN8o"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3mJwq3Gc2Tp"
      },
      "outputs": [],
      "source": [
        "unhelpful_phrases = [\n",
        "    \"I don't know, you should ask someone else.\",\n",
        "    \"You should go to a doctor immediately.\",\n",
        "    \"I can't help with medical questions.\",\n",
        "    \"Sorry, I'm not qualified to answer that.\",\n",
        "    \"Please consult a healthcare professional.\",\n",
        "    \"That's beyond my expertise.\",\n",
        "    \"I'm just an AI, I can't diagnose you.\",\n",
        "    \"Your question doesn't make sense.\",\n",
        "]\n",
        "curse_words = [\"damn\", \"hell\", \"crap\", \"stupid\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw8MJEp3cPq8"
      },
      "outputs": [],
      "source": [
        "def analyze_dataset(file_path: str) -> Dict[str, Any]:\n",
        "    global unhelpful_phrases, curse_words\n",
        "    # Cargamos el dataset con pandas\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    # Analizamos los duplicados.\n",
        "    duplicates = df.duplicated().sum()\n",
        "\n",
        "    # Analizamos si existen respuestas truncadas.\n",
        "    truncated = df[df['output'].str.endswith('...') |\n",
        "                  (df['output'].str.len() < 50) &\n",
        "                  (df['output'].str.len() > 0)].shape[0]\n",
        "    problematic = 0\n",
        "    for _, row in df.iterrows():\n",
        "        output = row.get('output', '')\n",
        "        if any(phrase in output.lower() for phrase in unhelpful_phrases) or \\\n",
        "           any(word in output.lower() for word in curse_words):\n",
        "            problematic += 1\n",
        "\n",
        "    return {\n",
        "        'total_entries': len(df),\n",
        "        'duplicates': duplicates,\n",
        "        'truncated_responses': truncated,\n",
        "        'problematic_content': problematic\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vjLxkwIcxnL"
      },
      "outputs": [],
      "source": [
        "def correct_dataset(input_file_path: str, output_file_path: str = None) -> None:\n",
        "    global unhelpful_phrases, curse_words\n",
        "    # Hacemos validación de si el archivo de salida es nulo.\n",
        "    if output_file_path is None:\n",
        "        dir_name = os.path.dirname(input_file_path)\n",
        "        base_name = os.path.basename(input_file_path)\n",
        "        output_file_path = os.path.join(dir_name, f\"corrected_{base_name}\")\n",
        "\n",
        "    # Volvemos a cargar el dataset.\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    # 1. Eliminamos los duplicados.\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    # 2. Filter out truncated responses\n",
        "    df = df[~(df['output'].str.endswith('...') | (df['output'].str.len() < 50))].reset_index(drop=True)\n",
        "\n",
        "    # 3. Eliminar el contenido potencialmente daniño\n",
        "    unhelpful_pattern = r\"|\".join(unhelpful_phrases)\n",
        "    curse_pattern = r\"\\b(\"+ \"\".join(curse_words) +\")\\b\"\n",
        "\n",
        "    # Filter out rows with problematic content\n",
        "    df = df[~(df['output'].str.contains(unhelpful_pattern, case=False, regex=True) |\n",
        "              df['output'].str.contains(curse_pattern, case=False, regex=True))].reset_index(drop=True)\n",
        "\n",
        "    # Save the corrected dataset\n",
        "    corrected_dataset = df.to_dict('records')\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(corrected_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Corrected dataset saved to {output_file_path}\")\n",
        "    print(f\"Original dataset size: {len(dataset)}, Corrected dataset size: {len(corrected_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URL del archivo JSON\n",
        "url = \"https://huggingface.co/datasets/medalpaca/medical_meadow_wikidoc/resolve/main/medical_meadow_wikidoc.json\"\n",
        "\n",
        "# Nombre del archivo local\n",
        "output_file = \"bad_medical_meadow_wikidoc.json\"\n",
        "\n",
        "# Descarga del archivo\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Archivo guardado como {output_file}\")\n",
        "else:\n",
        "    print(f\"Error al descargar: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wbsdWJ2dXnQ",
        "outputId": "085d7202-7c1d-4845-ce2e-5e08c784d9fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Analysis Results:\n",
            "Total entries: 11500\n",
            "Duplicates found: 1113\n",
            "Truncated responses found: 763\n",
            "Problematic content found: 283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-237358462.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df['output'].str.contains(curse_pattern, case=False, regex=True))].reset_index(drop=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected dataset saved to ./corrected_bad_medical_meadow_wikidoc.json\n",
            "Original dataset size: 11500, Corrected dataset size: 9641\n"
          ]
        }
      ],
      "source": [
        "input_file = \"./bad_medical_meadow_wikidoc.json\"\n",
        "\n",
        "analysis_results = analyze_dataset(input_file)\n",
        "print(\"Dataset Analysis Results:\")\n",
        "print(f\"Total entries: {analysis_results['total_entries']}\")\n",
        "print(f\"Duplicates found: {analysis_results['duplicates']}\")\n",
        "print(f\"Truncated responses found: {analysis_results['truncated_responses']}\")\n",
        "print(f\"Problematic content found: {analysis_results['problematic_content']}\")\n",
        "\n",
        "# Correct the dataset\n",
        "correct_dataset(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Limi1wjKdw4R"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_json(input_file)\n",
        "dataset = dataset[dataset['output'].str.len() < 500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrUxRfjGd__V"
      },
      "outputs": [],
      "source": [
        "result = []\n",
        "for _, row in dataset.iterrows():\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": row['instruction']},\n",
        "        {\"role\": \"user\", \"content\": row['input']},\n",
        "        {\"role\": \"assistant\", \"content\": row['output']}\n",
        "    ]\n",
        "    result.append({\"messages\": messages})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BESbjet0eaSg"
      },
      "outputs": [],
      "source": [
        "data_to_finetune = result[:500]\n",
        "# Para guardar en formato JSONL (JSON Lines)\n",
        "with open('./train_formatted_dataset.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for item in data_to_finetune:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81Hae5dNj-D8"
      },
      "outputs": [],
      "source": [
        "data_to_finetune = result[501:1000]\n",
        "# Para guardar en formato JSONL (JSON Lines)\n",
        "with open('./test_formatted_dataset.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for item in data_to_finetune:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "322Ch8omfgLt",
        "outputId": "34e45475-6607-49bf-aaca-ec3c56c529c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Znpdd--e_1J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V866Ti0w5iYh"
      },
      "outputs": [],
      "source": [
        "api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1meU1UXeRrR"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea9EZ9Xtfmvn"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "train = client.files.create(\n",
        "  file=open(\"train_formatted_dataset.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "test = client.files.create(\n",
        "  file=open(\"test_formatted_dataset.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBx4J-E8kG4a",
        "outputId": "23dc5b43-3c21-416a-e1b3-70ea85543c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train ID: file-JqgvG1NAVf19w5bQeJ76Nw\n",
            "Test ID: file-V7jgUevqPnJ1sHvYbq5PQ2\n"
          ]
        }
      ],
      "source": [
        "print(\"Train ID: \" + train.id)\n",
        "print(\"Test ID: \" + test.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJdR_ZhBfrK9"
      },
      "outputs": [],
      "source": [
        "job = client.fine_tuning.jobs.create(\n",
        "    training_file=\"file-JqgvG1NAVf19w5bQeJ76Nw\",\n",
        "    validation_file=\"file-V7jgUevqPnJ1sHvYbq5PQ2\",\n",
        "    model=\"gpt-4o-2024-08-06\",\n",
        "    method={\n",
        "        \"type\": \"supervised\",\n",
        "        \"supervised\": {\n",
        "            \"hyperparameters\": {\n",
        "            \"batch_size\": \"5\", # depende del problema, es un trade-off entre eficiencia en el uso de recursos y el performance del modelo.\n",
        "            \"learning_rate_multiplier\": \"0.001\", # Recomendado de 0.0001-10\n",
        "            \"n_epochs\": \"auto\",\n",
        "          }\n",
        "        },\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OTvi6PK3gqDu",
        "outputId": "5ba53d53-fc8b-49f3-90b1-86638a224de0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-fZyUZU0lVxhFtJE9IP4DhWrp', created_at=1745191343, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=5, learning_rate_multiplier=0.001, n_epochs=3), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-LYnt4yd3THU49Ltheyduajr1', result_files=[], seed=1057743196, status='cancelled', trained_tokens=None, training_file='file-GSattZMeuzLKCaGndq8XbG', validation_file=None, estimated_finish=1745192536, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=5, learning_rate_multiplier=0.001, n_epochs=3)), type='supervised'), user_provided_suffix=None)"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.fine_tuning.jobs.list(limit=10).data[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4tCB4KXSjFFU",
        "outputId": "c6f6f998-d70c-4ccf-f056-01bf04662ada"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-MabamF85lnuzvSfWzAdvIrT6', created_at=1745192387, level='warn', message='The job was stopped due to a cancellation request', object='fine_tuning.job.event', data=None, type='message'), FineTuningJobEvent(id='ftevent-y1hBhS1l0o2OihuE4YPPtHef', created_at=1745192321, level='info', message='Step 201/300: training loss=1.97', object='fine_tuning.job.event', data={'step': 201, 'train_loss': 1.9734954833984375, 'total_steps': 300, 'train_mean_token_accuracy': 0.6433333158493042}, type='metrics'), FineTuningJobEvent(id='ftevent-mjBGnOWRBqmiqYmpmsktC1va', created_at=1745192310, level='info', message='Step 200/300: training loss=2.68', object='fine_tuning.job.event', data={'step': 200, 'train_loss': 2.6830859184265137, 'total_steps': 300, 'train_mean_token_accuracy': 0.5175879597663879}, type='metrics'), FineTuningJobEvent(id='ftevent-05Jagepj3t8gI9plttJoJN2s', created_at=1745192304, level='info', message='Step 199/300: training loss=2.24', object='fine_tuning.job.event', data={'step': 199, 'train_loss': 2.2421655654907227, 'total_steps': 300, 'train_mean_token_accuracy': 0.6165803074836731}, type='metrics'), FineTuningJobEvent(id='ftevent-kextGTsVMhHdBi576LlNVY8p', created_at=1745192304, level='info', message='Step 198/300: training loss=2.89', object='fine_tuning.job.event', data={'step': 198, 'train_loss': 2.894522190093994, 'total_steps': 300, 'train_mean_token_accuracy': 0.6513761281967163}, type='metrics'), FineTuningJobEvent(id='ftevent-ycMzr0oD7FkzcJH5cAtDt0Ni', created_at=1745192304, level='info', message='Step 197/300: training loss=2.62', object='fine_tuning.job.event', data={'step': 197, 'train_loss': 2.6215171813964844, 'total_steps': 300, 'train_mean_token_accuracy': 0.5510835647583008}, type='metrics'), FineTuningJobEvent(id='ftevent-YcwHKM83Ybrc2hycPZPb2FLg', created_at=1745192300, level='info', message='Step 196/300: training loss=2.80', object='fine_tuning.job.event', data={'step': 196, 'train_loss': 2.8014028072357178, 'total_steps': 300, 'train_mean_token_accuracy': 0.5327510833740234}, type='metrics'), FineTuningJobEvent(id='ftevent-P9d3OjRLXyxOD6LF6A7clDQn', created_at=1745192300, level='info', message='Step 195/300: training loss=2.04', object='fine_tuning.job.event', data={'step': 195, 'train_loss': 2.038445472717285, 'total_steps': 300, 'train_mean_token_accuracy': 0.6181229948997498}, type='metrics'), FineTuningJobEvent(id='ftevent-A0MqcGavBddMs0htUVoxG37M', created_at=1745192300, level='info', message='Step 194/300: training loss=2.58', object='fine_tuning.job.event', data={'step': 194, 'train_loss': 2.577650785446167, 'total_steps': 300, 'train_mean_token_accuracy': 0.5797101259231567}, type='metrics'), FineTuningJobEvent(id='ftevent-fPsXvm0KIXo6BChReYgYLkEL', created_at=1745192299, level='info', message='Step 193/300: training loss=2.29', object='fine_tuning.job.event', data={'step': 193, 'train_loss': 2.286202907562256, 'total_steps': 300, 'train_mean_token_accuracy': 0.6075471639633179}, type='metrics'), FineTuningJobEvent(id='ftevent-J1QQZwLuqn7pEvqWHVLAoH62', created_at=1745192294, level='info', message='Step 192/300: training loss=2.43', object='fine_tuning.job.event', data={'step': 192, 'train_loss': 2.4330899715423584, 'total_steps': 300, 'train_mean_token_accuracy': 0.5984556078910828}, type='metrics'), FineTuningJobEvent(id='ftevent-C0vKJajnsG4I8UpefxuTY8uE', created_at=1745192294, level='info', message='Step 191/300: training loss=2.52', object='fine_tuning.job.event', data={'step': 191, 'train_loss': 2.518821954727173, 'total_steps': 300, 'train_mean_token_accuracy': 0.589958131313324}, type='metrics'), FineTuningJobEvent(id='ftevent-Ul3e8dvCxbcBaJqROJhniQU6', created_at=1745192293, level='info', message='Step 190/300: training loss=2.63', object='fine_tuning.job.event', data={'step': 190, 'train_loss': 2.629756450653076, 'total_steps': 300, 'train_mean_token_accuracy': 0.5652173757553101}, type='metrics'), FineTuningJobEvent(id='ftevent-x7ZXqAEm9t3h5PSTZHcoY0XI', created_at=1745192289, level='info', message='Step 189/300: training loss=2.79', object='fine_tuning.job.event', data={'step': 189, 'train_loss': 2.7932047843933105, 'total_steps': 300, 'train_mean_token_accuracy': 0.5687500238418579}, type='metrics'), FineTuningJobEvent(id='ftevent-svC6nQTReOnMAMzwl8BB2B4v', created_at=1745192289, level='info', message='Step 188/300: training loss=2.57', object='fine_tuning.job.event', data={'step': 188, 'train_loss': 2.5736451148986816, 'total_steps': 300, 'train_mean_token_accuracy': 0.5578034520149231}, type='metrics'), FineTuningJobEvent(id='ftevent-G2GsEdRfVrV8r6LYkY79bxdA', created_at=1745192289, level='info', message='Step 187/300: training loss=2.47', object='fine_tuning.job.event', data={'step': 187, 'train_loss': 2.4697084426879883, 'total_steps': 300, 'train_mean_token_accuracy': 0.6029411554336548}, type='metrics'), FineTuningJobEvent(id='ftevent-Jh76spzhAvCtZpcQ4oprKOEu', created_at=1745192284, level='info', message='Step 186/300: training loss=2.52', object='fine_tuning.job.event', data={'step': 186, 'train_loss': 2.516785144805908, 'total_steps': 300, 'train_mean_token_accuracy': 0.5263158082962036}, type='metrics'), FineTuningJobEvent(id='ftevent-TF3ffRPvkBHSpP9TNerU5QlM', created_at=1745192284, level='info', message='Step 185/300: training loss=2.92', object='fine_tuning.job.event', data={'step': 185, 'train_loss': 2.924294948577881, 'total_steps': 300, 'train_mean_token_accuracy': 0.5428571701049805}, type='metrics'), FineTuningJobEvent(id='ftevent-6Ar8lezWIfTV9cecqlU6sRsX', created_at=1745192284, level='info', message='Step 184/300: training loss=2.05', object='fine_tuning.job.event', data={'step': 184, 'train_loss': 2.052777051925659, 'total_steps': 300, 'train_mean_token_accuracy': 0.584022045135498}, type='metrics'), FineTuningJobEvent(id='ftevent-lXCEXoLcHPEcMcyaNF9Z4XOs', created_at=1745192283, level='info', message='Step 183/300: training loss=2.51', object='fine_tuning.job.event', data={'step': 183, 'train_loss': 2.5095901489257812, 'total_steps': 300, 'train_mean_token_accuracy': 0.5966851115226746}, type='metrics')], has_more=True, object='list')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-fZyUZU0lVxhFtJE9IP4DhWrp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[REGRESAR](../01_Fine_tuning_supervisado_de_GPT-4.md)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
